Il web scraping (detto anche web harvesting o web data extraction) è una tecnica informatica di estrazione di dati da un sito web per mezzo di programmi software. Di solito, tali programmi simulano la navigazione umana nel World Wide Web utilizzando l'Hypertext Transfer Protocol (HTTP) o attraverso browser, come Internet Explorer o Mozilla Firefox.

Strettamente correlato all'indicizzazione dei siti Internet, tale tecnica è attuata mediante l'uso di bot dalla maggior parte dei motori di ricerca. D'altro canto, il web scraping si concentra di più sulla trasformazione di dati non strutturati presenti in Rete, di solito in formato HTML, in metadati che possono essere memorizzati e analizzati in locale in un database. Il web harvesting è altresì affine alla web automation, che consiste nella simulazione della navigazione umana in Rete attraverso l'uso di software per computer.

Esistono metodi utilizzati da alcuni siti web per prevenire il web scraping, come ad esempio rilevare e impedire ai bot la visualizzazione delle loro pagine. Per aggirare il problema esistono sistemi di web scraping che si affidano a tecniche come DOM parsing, Computer Vision e natural language processing per simulare la navigazione web umana. Grazie a queste tecniche è possibile raccogliere i contenuti delle pagine web per l'analisi offline.[1] Può essere usato per confrontare prezzi online, monitorare dati meteorologici, rilevare modifiche in un sito internet, nella ricerca scientifica, per il web mashup e il web data integration.